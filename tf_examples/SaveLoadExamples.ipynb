{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading **parts** of TF models\n",
    "- How do you save a trained model and load only part of it into a new graph?\n",
    "- How do you load an entire trained model and train a new, modified version of it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first train a single layer neural network on the mnist dataset and save that to a file. Then we'll load that pretrained layer into a new graph with a 2-layer neural network, as the first layer. This network will then be fine-tuned on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13092543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'mldata.org dataset: mnist-original',\n",
       " 'COL_NAMES': ['label', 'data'],\n",
       " 'target': array([0., 0., 0., ..., 9., 9., 9.]),\n",
       " 'data': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = fetch_mldata(\"MNIST original\", data_home=\"~/data/mldata\")\n",
    "mnist[\"data\"] = mnist[\"data\"].astype(np.float32) / 255.\n",
    "print(mnist[\"data\"].mean())\n",
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into 2 halves\n",
    "perm = np.random.permutation(mnist[\"target\"].shape[0])\n",
    "mnist_1 = {\"data\": mnist[\"data\"][perm[0:35000]],\n",
    "           \"target\": mnist[\"target\"][perm[0:35000]]}\n",
    "mnist_2 = {\"data\": mnist[\"data\"][perm[35000:]],\n",
    "           \"target\": mnist[\"target\"][perm[35000:]]}\n",
    "\n",
    "# split both into train/valid sets\n",
    "perm_1 = np.random.permutation(mnist_1[\"target\"].shape[0])\n",
    "mnist_1_train = {\"data\": mnist_1[\"data\"][perm_1[0:31500]],\n",
    "                 \"target\": mnist_1[\"target\"][perm_1[0:31500]]}\n",
    "mnist_1_valid = {\"data\": mnist_1[\"data\"][perm_1[31500:]],\n",
    "                 \"target\": mnist_1[\"target\"][perm_1[31500:]]}\n",
    "\n",
    "perm_2 = np.random.permutation(mnist_2[\"target\"].shape[0])\n",
    "mnist_2_train = {\"data\": mnist_2[\"data\"][perm_2[0:31500]],\n",
    "                 \"target\": mnist_2[\"target\"][perm_2[0:31500]]}\n",
    "mnist_2_valid = {\"data\": mnist_2[\"data\"][perm_2[31500:]],\n",
    "                 \"target\": mnist_2[\"target\"][perm_2[31500:]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X_place, y_place, batch_size, num_epochs):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_place, y_place))\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.shuffle_and_repeat(5000, count=num_epochs))\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.map_and_batch(lambda x, y: (x, tf.one_hot(tf.cast(y, tf.int32), 10)), \n",
    "                                      batch_size,\n",
    "                                      num_parallel_batches=4))\n",
    "    dataset = dataset.prefetch(4)\n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared variables\n",
    "Variables have names that can be referenced when saving and restoring them. \n",
    "Since the `tf.train.Saver` object takes a list or dictionary of variables to save/restore, \n",
    "we can group the variables we want to share in a variable scope:\n",
    "```python\n",
    "with tf.variable_scope(\"shared\", reuse=tf.AUTO_REUSE):\n",
    "```\n",
    "The variable scope `shared` can be used accessed later via `tf.get_collection(..., scope=\"shared\")`.\n",
    "\n",
    "(**NOTE**: In this case, the `reuse` argument simply tells tensorflow to reuse variables that already \n",
    "exist with the same name. This is only important when calling create_model more than once in the\n",
    "same graph, not for restoring variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(version, iterator, mode):\n",
    "    \"\"\"\n",
    "    Returns ops for (pred_labels, loss, metrics)\n",
    "    \"\"\"\n",
    "    x, y = iterator.get_next()\n",
    "    \n",
    "    with tf.variable_scope(\"shared\", reuse=tf.AUTO_REUSE):\n",
    "\n",
    "        h = tf.layers.dense(x, 1000, activation=tf.nn.relu, name=\"dense_1\")\n",
    "\n",
    "    with tf.variable_scope(\"model\", reuse=tf.AUTO_REUSE):\n",
    "        if version == 2:\n",
    "            h = tf.layers.dense(h, 500, activation=tf.nn.relu, name=\"dense_2\")\n",
    "\n",
    "        logits = tf.layers.dense(h, 10, activation=tf.nn.relu, name=\"logits\")\n",
    "\n",
    "    crossent = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)\n",
    "    \n",
    "    loss = tf.reduce_sum(crossent)\n",
    "\n",
    "    if mode == \"TRAIN\":\n",
    "        opt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        update = opt.minimize(loss, name=\"update_op\")\n",
    "        metrics = None\n",
    "        metrics_updates = None\n",
    "        pred_labels = None\n",
    "    # metrics\n",
    "    elif mode == \"EVAL\":\n",
    "        update = None\n",
    "        total_loss, loss_update = tf.metrics.mean(values=loss)\n",
    "        pred_labels = tf.argmax(input=logits, axis=-1)\n",
    "        tgt_labels = tf.argmax(input=y, axis=-1)\n",
    "        acc, acc_update = tf.metrics.accuracy(predictions=pred_labels,\n",
    "                                            labels=tgt_labels)\n",
    "\n",
    "        # summaries\n",
    "        tf.summary.scalar(\"validation_loss\", total_loss, collections=[\"eval\"])\n",
    "        tf.summary.scalar(\"accuracy\", acc, collections=[\"eval\"])\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.name, var, collections=[\"eval\"])\n",
    "    \n",
    "        metrics = [total_loss, acc]\n",
    "        metrics_updates = [loss_update, acc_update]\n",
    "    \n",
    "    return pred_labels, loss, update, metrics, metrics_updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and restoring\n",
    "The `train` function contains the logic for both saving models and restoring\n",
    "(some) variables from a pretrained model. In this case, we train with one\n",
    "version of the model, then train again with a different version of the model.\n",
    "\n",
    "If `save_path` is specified, then variables from the scope `shared` will be\n",
    "loaded from the checkpoint:\n",
    "```python\n",
    "if save_path is not None:\n",
    "    loader = tf.train.Saver(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"shared\"))\n",
    "    # ...\n",
    "    loader.restore(sess, save_path)\n",
    "```\n",
    "This works because `create_model` will name the variables the same way. \n",
    "\n",
    "**NOTE**: Restoring variables will initialize them, but any variables that are not restored\n",
    "must be initialized as normal. Here, all variables are initialized, and any that are\n",
    "restored are restored after this initialization. **If variables are initialized after they\n",
    "are restored, they will be overwritten.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mnist_train, mnist_valid, logdir, save_path=None):\n",
    "    X_placeholder = tf.placeholder(mnist_train[\"data\"].dtype, shape=mnist_train[\"data\"].shape)\n",
    "    y_placeholder = tf.placeholder(mnist_train[\"target\"].dtype, shape=mnist_train[\"target\"].shape)\n",
    "    train_dataset = create_dataset(X_placeholder, y_placeholder, 32, 3)\n",
    "    train_iterator = train_dataset.make_initializable_iterator()\n",
    "\n",
    "    version = 1 if save_path is None else 2\n",
    "    \n",
    "    _, _, train_op, _, _ = create_model(version, train_iterator, \"TRAIN\")\n",
    "\n",
    "    valid_X_placeholder = tf.placeholder(mnist_valid[\"data\"].dtype, shape=mnist_valid[\"data\"].shape)\n",
    "    valid_y_placeholder = tf.placeholder(mnist_valid[\"target\"].dtype, shape=mnist_valid[\"target\"].shape)\n",
    "    valid_dataset = create_dataset(valid_X_placeholder, valid_y_placeholder, 64, 1)\n",
    "    valid_iterator = valid_dataset.make_initializable_iterator()\n",
    "    \n",
    "    pred_y_op, loss_op, _, metrics, metrics_updates = create_model(version, valid_iterator, \"EVAL\")\n",
    "    loss_update_op = metrics_updates[0]\n",
    "    acc_update_op = metrics_updates[1]\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    local_init_op = tf.local_variables_initializer()\n",
    "    eval_summaries = tf.summary.merge_all(\"eval\")\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    \n",
    "    if save_path is not None:\n",
    "        loader = tf.train.Saver(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"shared\"))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run([init_op])\n",
    "        \n",
    "        if save_path is not None:\n",
    "            loader.restore(sess, save_path)\n",
    "        \n",
    "        sess.run(train_iterator.initializer, \n",
    "                 feed_dict={X_placeholder: mnist_train[\"data\"],\n",
    "                            y_placeholder: mnist_train[\"target\"]})\n",
    "        step = 0\n",
    "        while True:\n",
    "            try:\n",
    "                step += 1\n",
    "                _ = sess.run(train_op)\n",
    "                if step % 50 == 0:\n",
    "                    # VALIDATION\n",
    "                    sess.run(valid_iterator.initializer,\n",
    "                             feed_dict={valid_X_placeholder: mnist_valid[\"data\"],\n",
    "                                        valid_y_placeholder: mnist_valid[\"target\"]})\n",
    "                    sess.run([local_init_op])\n",
    "                    while True:\n",
    "                        try:\n",
    "                            avg_loss, acc = sess.run([loss_update_op, acc_update_op])\n",
    "                        except tf.errors.OutOfRangeError:\n",
    "                            print(\"Step %d: Loss %3.3f, Accuracy: %.4f\" % \n",
    "                                 (step, avg_loss, acc))\n",
    "                            summ = sess.run(eval_summaries)\n",
    "                            summary_writer.add_summary(summ, step)\n",
    "                            break\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Training done.\")\n",
    "                summary_writer.flush()\n",
    "                summary_writer.close()\n",
    "                break\n",
    "\n",
    "        # save the model\n",
    "        if version == 1:\n",
    "            ckpt_path = saver.save(sess, \"/tmp/models/model.ckpt\")\n",
    "            return ckpt_path\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name shared/dense_1/kernel:0 is illegal; using shared/dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name shared/dense_1/bias:0 is illegal; using shared/dense_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name model/logits/kernel:0 is illegal; using model/logits/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name model/logits/bias:0 is illegal; using model/logits/bias_0 instead.\n",
      "Step 50: Loss 113.589, Accuracy: 0.4329\n",
      "Step 100: Loss 96.301, Accuracy: 0.5286\n",
      "Step 150: Loss 79.204, Accuracy: 0.5934\n",
      "Step 200: Loss 76.312, Accuracy: 0.6126\n",
      "Step 250: Loss 84.846, Accuracy: 0.5677\n",
      "Step 300: Loss 76.181, Accuracy: 0.6097\n",
      "Step 350: Loss 62.318, Accuracy: 0.6797\n",
      "Step 400: Loss 53.678, Accuracy: 0.7489\n",
      "Step 450: Loss 47.602, Accuracy: 0.8074\n",
      "Step 500: Loss 44.714, Accuracy: 0.8209\n",
      "Step 550: Loss 46.082, Accuracy: 0.8117\n",
      "Step 600: Loss 30.508, Accuracy: 0.8340\n",
      "Step 650: Loss 27.289, Accuracy: 0.8489\n",
      "Step 700: Loss 27.659, Accuracy: 0.8446\n",
      "Step 750: Loss 27.034, Accuracy: 0.8469\n",
      "Step 800: Loss 24.954, Accuracy: 0.8583\n",
      "Step 850: Loss 26.741, Accuracy: 0.8463\n",
      "Step 900: Loss 24.055, Accuracy: 0.8574\n",
      "Step 950: Loss 26.092, Accuracy: 0.8486\n",
      "Step 1000: Loss 23.487, Accuracy: 0.8651\n",
      "Step 1050: Loss 24.487, Accuracy: 0.8606\n",
      "Step 1100: Loss 23.688, Accuracy: 0.8617\n",
      "Step 1150: Loss 24.085, Accuracy: 0.8569\n",
      "Step 1200: Loss 23.978, Accuracy: 0.8617\n",
      "Step 1250: Loss 22.788, Accuracy: 0.8671\n",
      "Step 1300: Loss 25.351, Accuracy: 0.8517\n",
      "Step 1350: Loss 22.702, Accuracy: 0.8663\n",
      "Step 1400: Loss 22.781, Accuracy: 0.8626\n",
      "Step 1450: Loss 23.642, Accuracy: 0.8609\n",
      "Step 1500: Loss 23.311, Accuracy: 0.8586\n",
      "Step 1550: Loss 22.094, Accuracy: 0.8689\n",
      "Step 1600: Loss 22.584, Accuracy: 0.8660\n",
      "Step 1650: Loss 23.195, Accuracy: 0.8606\n",
      "Step 1700: Loss 21.948, Accuracy: 0.8680\n",
      "Step 1750: Loss 22.248, Accuracy: 0.8660\n",
      "Step 1800: Loss 21.052, Accuracy: 0.8751\n",
      "Step 1850: Loss 21.659, Accuracy: 0.8694\n",
      "Step 1900: Loss 21.759, Accuracy: 0.8689\n",
      "Step 1950: Loss 20.960, Accuracy: 0.8743\n",
      "Step 2000: Loss 20.913, Accuracy: 0.8729\n",
      "Step 2050: Loss 22.281, Accuracy: 0.8663\n",
      "Step 2100: Loss 21.880, Accuracy: 0.8697\n",
      "Step 2150: Loss 21.547, Accuracy: 0.8697\n",
      "Step 2200: Loss 20.566, Accuracy: 0.8743\n",
      "Step 2250: Loss 21.328, Accuracy: 0.8686\n",
      "Step 2300: Loss 21.268, Accuracy: 0.8697\n",
      "Step 2350: Loss 20.910, Accuracy: 0.8720\n",
      "Step 2400: Loss 21.097, Accuracy: 0.8714\n",
      "Step 2450: Loss 21.169, Accuracy: 0.8714\n",
      "Step 2500: Loss 20.888, Accuracy: 0.8743\n",
      "Step 2550: Loss 21.126, Accuracy: 0.8694\n",
      "Step 2600: Loss 20.813, Accuracy: 0.8717\n",
      "Step 2650: Loss 20.518, Accuracy: 0.8774\n",
      "Step 2700: Loss 20.617, Accuracy: 0.8729\n",
      "Step 2750: Loss 20.785, Accuracy: 0.8737\n",
      "Step 2800: Loss 21.689, Accuracy: 0.8689\n",
      "Step 2850: Loss 21.019, Accuracy: 0.8720\n",
      "Step 2900: Loss 20.054, Accuracy: 0.8760\n",
      "Step 2950: Loss 21.349, Accuracy: 0.8709\n",
      "Training done.\n"
     ]
    }
   ],
   "source": [
    "# train the first model\n",
    "ckpt_path = train(mnist_1_train, mnist_1_valid, \"/tmp/models/shared/model1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name shared/dense_1/kernel:0 is illegal; using shared/dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name shared/dense_1/bias:0 is illegal; using shared/dense_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name model/dense_2/kernel:0 is illegal; using model/dense_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name model/dense_2/bias:0 is illegal; using model/dense_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name model/logits/kernel:0 is illegal; using model/logits/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name model/logits/bias:0 is illegal; using model/logits/bias_0 instead.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/models/model.ckpt\n",
      "Step 50: Loss 52.512, Accuracy: 0.6817\n",
      "Step 100: Loss 51.127, Accuracy: 0.6891\n",
      "Step 150: Loss 45.964, Accuracy: 0.7423\n",
      "Step 200: Loss 34.847, Accuracy: 0.7891\n",
      "Step 250: Loss 34.135, Accuracy: 0.7911\n",
      "Step 300: Loss 34.662, Accuracy: 0.7897\n",
      "Step 350: Loss 34.207, Accuracy: 0.7917\n",
      "Step 400: Loss 35.034, Accuracy: 0.7880\n",
      "Step 450: Loss 34.497, Accuracy: 0.7874\n",
      "Step 500: Loss 34.397, Accuracy: 0.7894\n",
      "Step 550: Loss 34.452, Accuracy: 0.7883\n",
      "Step 600: Loss 34.894, Accuracy: 0.7846\n",
      "Step 650: Loss 33.311, Accuracy: 0.7949\n",
      "Step 700: Loss 33.327, Accuracy: 0.7917\n",
      "Step 750: Loss 33.210, Accuracy: 0.7946\n",
      "Step 800: Loss 25.780, Accuracy: 0.8717\n",
      "Step 850: Loss 20.662, Accuracy: 0.8777\n",
      "Step 900: Loss 21.721, Accuracy: 0.8723\n",
      "Step 950: Loss 20.653, Accuracy: 0.8774\n",
      "Step 1000: Loss 21.010, Accuracy: 0.8737\n",
      "Step 1050: Loss 20.099, Accuracy: 0.8760\n",
      "Step 1100: Loss 21.235, Accuracy: 0.8757\n",
      "Step 1150: Loss 21.184, Accuracy: 0.8726\n",
      "Step 1200: Loss 20.956, Accuracy: 0.8734\n",
      "Step 1250: Loss 20.456, Accuracy: 0.8786\n",
      "Step 1300: Loss 20.040, Accuracy: 0.8746\n",
      "Step 1350: Loss 19.655, Accuracy: 0.8809\n",
      "Step 1400: Loss 19.579, Accuracy: 0.8820\n",
      "Step 1450: Loss 19.538, Accuracy: 0.8797\n",
      "Step 1500: Loss 20.473, Accuracy: 0.8771\n",
      "Step 1550: Loss 19.872, Accuracy: 0.8809\n",
      "Step 1600: Loss 21.464, Accuracy: 0.8803\n",
      "Step 1650: Loss 19.511, Accuracy: 0.8809\n",
      "Step 1700: Loss 19.504, Accuracy: 0.8794\n",
      "Step 1750: Loss 19.895, Accuracy: 0.8803\n",
      "Step 1800: Loss 19.871, Accuracy: 0.8800\n",
      "Step 1850: Loss 19.235, Accuracy: 0.8817\n",
      "Step 1900: Loss 19.948, Accuracy: 0.8786\n",
      "Step 1950: Loss 19.342, Accuracy: 0.8834\n",
      "Step 2000: Loss 19.144, Accuracy: 0.8803\n",
      "Step 2050: Loss 19.518, Accuracy: 0.8806\n",
      "Step 2100: Loss 18.884, Accuracy: 0.8826\n",
      "Step 2150: Loss 19.400, Accuracy: 0.8834\n",
      "Step 2200: Loss 19.686, Accuracy: 0.8803\n",
      "Step 2250: Loss 19.299, Accuracy: 0.8826\n",
      "Step 2300: Loss 19.878, Accuracy: 0.8783\n",
      "Step 2350: Loss 18.819, Accuracy: 0.8834\n",
      "Step 2400: Loss 18.565, Accuracy: 0.8857\n",
      "Step 2450: Loss 19.201, Accuracy: 0.8831\n",
      "Step 2500: Loss 20.062, Accuracy: 0.8777\n",
      "Step 2550: Loss 19.204, Accuracy: 0.8849\n",
      "Step 2600: Loss 18.852, Accuracy: 0.8814\n",
      "Step 2650: Loss 18.998, Accuracy: 0.8800\n",
      "Step 2700: Loss 18.834, Accuracy: 0.8837\n",
      "Step 2750: Loss 18.869, Accuracy: 0.8826\n",
      "Step 2800: Loss 18.803, Accuracy: 0.8823\n",
      "Step 2850: Loss 18.948, Accuracy: 0.8837\n",
      "Step 2900: Loss 18.857, Accuracy: 0.8851\n",
      "Step 2950: Loss 19.385, Accuracy: 0.8854\n",
      "Training done.\n"
     ]
    }
   ],
   "source": [
    "# now create a new, 2-layer network and load the weights from the first\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train(mnist_2_train, mnist_2_valid, \"/tmp/models/shared/model2\", ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
